{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/111Vidya/AI-Fundametals/blob/main/AI_Jun4_RNN_LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDsnvZanwmuD",
        "outputId": "30aa11d3-d685-4fe6-8b56-35ccbe5be6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               53200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213301 (833.21 KB)\n",
            "Trainable params: 213301 (833.21 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "391/391 [==============================] - 282s 711ms/step - loss: 0.4841 - accuracy: 0.7650\n",
            "Accuracy: 83.21%\n"
          ]
        }
      ],
      "source": [
        "# LSTM for sequence classification in the IMDB dataset\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential # sequence model\n",
        "from keras.layers import Dense # hidden layers\n",
        "from keras.layers import LSTM # LSTM model\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "# load the dataset but only keep the top n words, zero the rest. Senti Analysis\n",
        "top_words = 5000 # only 500 top word from the dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500 # length restricting to 500 words\n",
        "X_train = pad_sequences(X_train, maxlen=max_review_length) # pad to understand better and every corner of the reviews\n",
        "X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32  #batch size\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length)) #3 defined layers\n",
        "model.add(LSTM(100)) # 100 cells\n",
        "model.add(Dense(1, activation='sigmoid')) #output layer 1, sigmoid -Binary classification\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #define metrics/model performance\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acc 83.21%\n",
        "Embedding vector length - to create a vector(numbers) for words in the review to clasiffy to positive or negetive and truncate the length. Pass the embedded layer thru NN(LSTM). 100 LSTM cells will analyse the predictions that will be consolidated."
      ],
      "metadata": {
        "id": "-kegRScj3pWf"
      }
    }
  ]
}